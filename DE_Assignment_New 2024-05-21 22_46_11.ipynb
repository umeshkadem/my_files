{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f833c26a-9872-4458-8385-0f9e745a5f68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting snowflake-connector-python\n  Downloading snowflake_connector_python-3.10.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\nCollecting asn1crypto<2.0.0,>0.24.0\n  Downloading asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\nCollecting pyjwt<3.0.0\n  Downloading PyJWT-2.8.0-py3-none-any.whl (22 kB)\nCollecting pyOpenSSL<25.0.0,>=16.2.0\n  Downloading pyOpenSSL-24.1.0-py3-none-any.whl (56 kB)\nRequirement already satisfied: filelock<4,>=3.5 in /usr/local/lib/python3.9/dist-packages (from snowflake-connector-python) (3.9.0)\nCollecting tomlkit\n  Downloading tomlkit-0.12.5-py3-none-any.whl (37 kB)\nRequirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /usr/local/lib/python3.9/dist-packages (from snowflake-connector-python) (2.6.2)\nRequirement already satisfied: cryptography<43.0.0,>=3.1.0 in /databricks/python3/lib/python3.9/site-packages (from snowflake-connector-python) (3.4.8)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.9/site-packages (from snowflake-connector-python) (2.0.4)\nRequirement already satisfied: pytz in /databricks/python3/lib/python3.9/site-packages (from snowflake-connector-python) (2021.3)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from snowflake-connector-python) (2021.10.8)\nRequirement already satisfied: urllib3<2.0.0,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from snowflake-connector-python) (1.26.9)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from snowflake-connector-python) (3.3)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.9/site-packages (from snowflake-connector-python) (21.3)\nCollecting typing-extensions<5,>=4.3\n  Downloading typing_extensions-4.12.0-py3-none-any.whl (37 kB)\nRequirement already satisfied: cffi<2.0.0,>=1.9 in /databricks/python3/lib/python3.9/site-packages (from snowflake-connector-python) (1.15.0)\nCollecting sortedcontainers>=2.4.0\n  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\nRequirement already satisfied: requests<3.0.0 in /databricks/python3/lib/python3.9/site-packages (from snowflake-connector-python) (2.27.1)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.9/site-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python) (2.21)\nCollecting cryptography<43.0.0,>=3.1.0\n  Downloading cryptography-42.0.7-cp39-abi3-manylinux_2_28_x86_64.whl (3.8 MB)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.9/site-packages (from packaging->snowflake-connector-python) (3.0.4)\nInstalling collected packages: cryptography, typing-extensions, tomlkit, sortedcontainers, pyOpenSSL, pyjwt, asn1crypto, snowflake-connector-python\n  Attempting uninstall: cryptography\n    Found existing installation: cryptography 3.4.8\n    Not uninstalling cryptography at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-b4ec2569-8b99-4435-9327-d0a392b163b8\n    Can't uninstall 'cryptography'. No files were found to uninstall.\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing-extensions 4.1.1\n    Not uninstalling typing-extensions at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-b4ec2569-8b99-4435-9327-d0a392b163b8\n    Can't uninstall 'typing-extensions'. No files were found to uninstall.\nSuccessfully installed asn1crypto-1.5.1 cryptography-42.0.7 pyOpenSSL-24.1.0 pyjwt-2.8.0 snowflake-connector-python-3.10.1 sortedcontainers-2.4.0 tomlkit-0.12.5 typing-extensions-4.12.0\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "pip install snowflake-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f54d4fe-c01d-454c-bad2-6c7546e9a6c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n|C_CUSTOMER_ID   |\n+----------------+\n|AAAAAAAAAAAAAEBA|\n|AAAAAAAAAAAAAJAA|\n|AAAAAAAAAAAAALDA|\n|AAAAAAAAAAAAAMDA|\n|AAAAAAAAAAAABHCA|\n|AAAAAAAAAAAACBBA|\n|AAAAAAAAAAAACBCA|\n|AAAAAAAAAAAACCAA|\n|AAAAAAAAAAAACCBA|\n|AAAAAAAAAAAACEBA|\n|AAAAAAAAAAAACGCA|\n|AAAAAAAAAAAACLCA|\n|AAAAAAAAAAAACPCA|\n|AAAAAAAAAAAACPCA|\n|AAAAAAAAAAAADBAA|\n|AAAAAAAAAAAADDBA|\n|AAAAAAAAAAAADFDA|\n|AAAAAAAAAAAADHDA|\n|AAAAAAAAAAAADKAA|\n|AAAAAAAAAAAADKDA|\n|AAAAAAAAAAAAEECA|\n|AAAAAAAAAAAAEKBA|\n|AAAAAAAAAAAAELAA|\n|AAAAAAAAAAAAEPBA|\n|AAAAAAAAAAAAFBCA|\n|AAAAAAAAAAAAFCCA|\n|AAAAAAAAAAAAFEAA|\n|AAAAAAAAAAAAFEBA|\n|AAAAAAAAAAAAFEBA|\n|AAAAAAAAAAAAFIDA|\n|AAAAAAAAAAAAFKBA|\n|AAAAAAAAAAAAFOBA|\n|AAAAAAAAAAAAGEBA|\n|AAAAAAAAAAAAGEBA|\n|AAAAAAAAAAAAHDCA|\n|AAAAAAAAAAAAHJBA|\n|AAAAAAAAAAAAHJDA|\n|AAAAAAAAAAAAIABA|\n|AAAAAAAAAAAAIDAA|\n|AAAAAAAAAAAAINDA|\n|AAAAAAAAAAAAJABA|\n|AAAAAAAAAAAAJBBA|\n|AAAAAAAAAAAAJCAA|\n|AAAAAAAAAAAAJDAA|\n|AAAAAAAAAAAAJEAA|\n|AAAAAAAAAAAAJHBA|\n|AAAAAAAAAAAAJHDA|\n|AAAAAAAAAAAAJJBA|\n|AAAAAAAAAAAAJKBA|\n|AAAAAAAAAAAAJOCA|\n|AAAAAAAAAAAAKADA|\n|AAAAAAAAAAAAKCDA|\n|AAAAAAAAAAAAKFBA|\n|AAAAAAAAAAAAKGDA|\n|AAAAAAAAAAAAKHBA|\n|AAAAAAAAAAAAKPCA|\n|AAAAAAAAAAAALBDA|\n|AAAAAAAAAAAALECA|\n|AAAAAAAAAAAALGBA|\n|AAAAAAAAAAAALGCA|\n|AAAAAAAAAAAALMBA|\n|AAAAAAAAAAAAMDBA|\n|AAAAAAAAAAAANABA|\n|AAAAAAAAAAAANDCA|\n|AAAAAAAAAAAANEAA|\n|AAAAAAAAAAAANECA|\n|AAAAAAAAAAAANGAA|\n|AAAAAAAAAAAANJDA|\n|AAAAAAAAAAAANLAA|\n|AAAAAAAAAAAANLBA|\n|AAAAAAAAAAAANPCA|\n|AAAAAAAAAAAAOCBA|\n|AAAAAAAAAAAAOECA|\n|AAAAAAAAAAAAPDAA|\n|AAAAAAAAAAABABCA|\n|AAAAAAAAAAABACBA|\n|AAAAAAAAAAABADBA|\n|AAAAAAAAAAABAHBA|\n|AAAAAAAAAAABALDA|\n|AAAAAAAAAAABBAAA|\n|AAAAAAAAAAABBBDA|\n|AAAAAAAAAAABBGBA|\n|AAAAAAAAAAABBGDA|\n|AAAAAAAAAAABBKBA|\n|AAAAAAAAAAABBLAA|\n|AAAAAAAAAAABBNAA|\n|AAAAAAAAAAABBPCA|\n|AAAAAAAAAAABCDBA|\n|AAAAAAAAAAABCIBA|\n|AAAAAAAAAAABCJBA|\n|AAAAAAAAAAABCPCA|\n|AAAAAAAAAAABDFBA|\n|AAAAAAAAAAABDHCA|\n|AAAAAAAAAAABDLCA|\n|AAAAAAAAAAABDMAA|\n|AAAAAAAAAAABELBA|\n|AAAAAAAAAAABEMBA|\n|AAAAAAAAAAABFHDA|\n|AAAAAAAAAAABFKBA|\n|AAAAAAAAAAABGAAA|\n+----------------+\nonly showing top 100 rows\n\nNumber of filtered customers: 5146421\n+----------------+\n|   C_CUSTOMER_ID|\n+----------------+\n|AAAAAAAAAAAAAEBA|\n+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# query:1\n",
    "\n",
    "sfOptions = {\n",
    "  \"sfURL\" : \"https://uh98701.ap-southeast-1.snowflakecomputing.com\",\n",
    "  \"sfAccount\" : \"uh98701.ap-southeast-1\",\n",
    "  \"sfUser\" : \"UMESHCHANDRAKADEM\",\n",
    "  \"sfPassword\" : \"Umesh.K@127\",\n",
    "  \"sfDatabase\" : \"SNOWFLAKE_SAMPLE_DATA\",\n",
    "  \"sfSchema\" : \"TPCDS_SF10TCL\",\n",
    "  \"sfWarehouse\" : \"COMPUTE_WH\"\n",
    "}\n",
    "\n",
    "# Create a SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg\n",
    "spark = SparkSession.builder \\\n",
    ".appName(\"Customer Total Return Analysis\") \\\n",
    ".getOrCreate()\n",
    "\n",
    "# Read data from Snowflake into Spark DataFrame\n",
    "store_returns_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"store_returns\").load()\n",
    "date_dim_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"date_dim\").load()\n",
    "store_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"store\").load()\n",
    "customer_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"customer\").load()\n",
    "\n",
    "# Perform necessary transformations\n",
    "joined_data = store_returns_df.join(date_dim_df, store_returns_df[\"SR_RETURNED_DATE_SK\"] == date_dim_df[\"D_DATE_SK\"]) \\\n",
    ".filter(date_dim_df[\"D_YEAR\"] == 2000) \\\n",
    ".select(\"SR_CUSTOMER_SK\", \"SR_STORE_SK\", \"SR_RETURN_AMT\")\n",
    " \n",
    "customer_total_return = joined_data.groupBy(\"SR_CUSTOMER_SK\", \"SR_STORE_SK\") \\\n",
    ".sum(\"SR_RETURN_AMT\") \\\n",
    ".withColumnRenamed(\"SR_CUSTOMER_SK\", \"CTR_CUSTOMER_SK\") \\\n",
    ".withColumnRenamed(\"SR_STORE_SK\", \"CTR_STORE_SK\") \\\n",
    ".withColumnRenamed(\"sum(SR_RETURN_AMT)\", \"CTR_TOTAL_RETURN\")\n",
    " \n",
    "avg_return_per_store = customer_total_return.groupBy(\"CTR_STORE_SK\") \\\n",
    ".agg((avg(\"CTR_TOTAL_RETURN\") * 1.2).alias(\"AVG_TOTAL_RETURN\"))\n",
    "\n",
    "# Filter customers based on the condition\n",
    "filtered_customers = customer_total_return.join(store_df, store_df[\"S_STORE_SK\"] == customer_total_return[\"CTR_STORE_SK\"]) \\\n",
    ".join(customer_df, customer_df[\"C_CUSTOMER_SK\"] == customer_total_return[\"CTR_CUSTOMER_SK\"]) \\\n",
    ".join(avg_return_per_store, avg_return_per_store[\"CTR_STORE_SK\"] == customer_total_return[\"CTR_STORE_SK\"]) \\\n",
    ".filter(customer_total_return[\"CTR_TOTAL_RETURN\"] > avg_return_per_store[\"AVG_TOTAL_RETURN\"]) \\\n",
    ".filter(store_df[\"S_STATE\"] == \"TN\") \\\n",
    ".select(\"C_CUSTOMER_ID\") \\\n",
    ".orderBy(\"C_CUSTOMER_ID\") \n",
    "\n",
    "try:\n",
    "    filtered_customers.show(n=100, truncate=False) # Adjust 'n' to the desired number of rows\n",
    "except Exception as e:\n",
    "    print(\"An error occurred while showing the result:\", e)\n",
    "\n",
    "# Apply the filter and count the rows\n",
    "filtered_customers_count = filtered_customers.distinct().count()\n",
    "\n",
    "# Print the count of filtered customers\n",
    "print(\"Number of filtered customers:\", filtered_customers_count)\n",
    "\n",
    "\n",
    "# filter of record\n",
    "filtered_df = filtered_customers.filter(filtered_customers[\"C_CUSTOMER_ID\"] == \"AAAAAAAAAAAAAEBA\")\n",
    "\n",
    "# Show filtered DataFrame\n",
    "filtered_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7218911-e61b-4109-b77e-a4cace2c7a93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n|d_week_seq1|sun_sales_ratio|mon_sales_ratio|tue_sales_ratio|wed_sales_ratio|thu_sales_ratio|fri_sales_ratio|sat_sales_ratio|\n+-----------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n|       5283|           1.00|           1.00|           1.00|           1.00|           1.00|           1.00|           1.00|\n|       5283|           1.00|           1.00|           1.00|           1.00|           1.00|           1.00|           1.00|\n|       5283|           1.00|           1.00|           1.00|           1.00|           1.00|           1.00|           1.00|\n|       5283|           1.00|           1.00|           1.00|           1.00|           1.00|           1.00|           1.00|\n|       5283|           1.00|           1.00|           1.00|           1.00|           1.00|           1.00|           1.00|\n|       5283|           1.00|           1.00|           1.00|           1.00|           1.00|           1.00|           1.00|\n|       5283|           1.00|           1.00|           1.00|           1.00|           1.00|           1.00|           1.00|\n|       5283|           1.00|           1.00|           1.00|           1.00|           1.00|           1.00|           1.00|\n|       5283|           1.00|           1.00|           1.00|           1.00|           1.00|           1.00|           1.00|\n|       5283|           1.00|           1.00|           1.00|           1.00|           1.00|           1.00|           1.00|\n|       5283|           1.00|           1.00|           1.00|           1.00|           1.00|           1.00|           1.00|\n|       5283|           1.00|           1.00|           1.00|           1.00|           1.00|           1.00|           1.00|\n|       5283|           1.00|           1.00|           1.00|           1.00|           1.00|           1.00|           1.00|\n|       5283|           1.00|           1.00|           1.00|           1.00|           1.00|           1.00|           1.00|\n|       5283|           1.00|           1.00|           1.00|           1.00|           1.00|           1.00|           1.00|\n|       5283|           1.00|           1.00|           1.00|           1.00|           1.00|           1.00|           1.00|\n|       5283|           1.00|           1.00|           1.00|           1.00|           1.00|           1.00|           1.00|\n|       5283|           1.00|           1.00|           1.00|           1.00|           1.00|           1.00|           1.00|\n|       5283|           1.00|           1.00|           1.00|           1.00|           1.00|           1.00|           1.00|\n|       5283|           1.00|           1.00|           1.00|           1.00|           1.00|           1.00|           1.00|\n+-----------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\nonly showing top 20 rows\n\n+------------------+\n|count(d_week_seq1)|\n+------------------+\n|              2513|\n+------------------+\n\n+-----------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n|d_week_seq1|sun_sales_ratio|mon_sales_ratio|tue_sales_ratio|wed_sales_ratio|thu_sales_ratio|fri_sales_ratio|sat_sales_ratio|\n+-----------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n|       5270|           3.44|           1.82|           1.89|           1.60|           3.45|           3.44|           3.44|\n|       5270|           3.44|           1.82|           1.89|           1.60|           3.45|           3.44|           3.44|\n|       5270|           3.44|           1.82|           1.89|           1.60|           3.45|           3.44|           3.44|\n|       5270|           3.44|           1.82|           1.89|           1.60|           3.45|           3.44|           3.44|\n|       5270|           3.44|           1.82|           1.89|           1.60|           3.45|           3.44|           3.44|\n|       5270|           3.44|           1.82|           1.89|           1.60|           3.45|           3.44|           3.44|\n|       5270|           3.44|           1.82|           1.89|           1.60|           3.45|           3.44|           3.44|\n+-----------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# 2.query:\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, round, sum, when, count\n",
    " \n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"Weekly Sales Increase Analysis\").getOrCreate()\n",
    " \n",
    "# Read web_sales and catalog_sales into DataFrame\n",
    "web_sales_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"web_sales\").load()\n",
    "catalog_sales_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"catalog_sales\").load()\n",
    "date_dim_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"date_dim\").load()\n",
    " \n",
    "# Combine web_sales and catalog_sales into a single DataFrame with common columns\n",
    "wscs_df = web_sales_df.select(\"WS_SOLD_DATE_SK\", \"WS_EXT_SALES_PRICE\").unionAll(catalog_sales_df.select(\"CS_SOLD_DATE_SK\", \"CS_EXT_SALES_PRICE\")).withColumnRenamed(\"WS_SOLD_DATE_SK\", \"sold_date_sk\").withColumnRenamed(\"WS_EXT_SALES_PRICE\", \"sales_price\")\n",
    " \n",
    "# Join with date_dim DataFrame to get day name and week sequence\n",
    "wswscs_df = wscs_df.join(date_dim_df.alias(\"date_dim_join\"), wscs_df[\"sold_date_sk\"] == col(\"date_dim_join.D_DATE_SK\")) \\\n",
    "           .groupBy(\"date_dim_join.D_WEEK_SEQ\") \\\n",
    "           .agg(\n",
    "               round(sum(when(col(\"date_dim_join.D_DAY_NAME\") == \"Sunday\", col(\"sales_price\")).otherwise(0)), 2).alias(\"sun_sales\"),\n",
    "               round(sum(when(col(\"date_dim_join.D_DAY_NAME\") == \"Monday\", col(\"sales_price\")).otherwise(0)), 2).alias(\"mon_sales\"),\n",
    "               round(sum(when(col(\"date_dim_join.D_DAY_NAME\") == \"Tuesday\", col(\"sales_price\")).otherwise(0)), 2).alias(\"tue_sales\"),\n",
    "               round(sum(when(col(\"date_dim_join.D_DAY_NAME\") == \"Wednesday\", col(\"sales_price\")).otherwise(0)), 2).alias(\"wed_sales\"),\n",
    "               round(sum(when(col(\"date_dim_join.D_DAY_NAME\") == \"Thursday\", col(\"sales_price\")).otherwise(0)), 2).alias(\"thu_sales\"),\n",
    "               round(sum(when(col(\"date_dim_join.D_DAY_NAME\") == \"Friday\", col(\"sales_price\")).otherwise(0)), 2).alias(\"fri_sales\"),\n",
    "               round(sum(when(col(\"date_dim_join.D_DAY_NAME\") == \"Saturday\", col(\"sales_price\")).otherwise(0)), 2).alias(\"sat_sales\")\n",
    "           )\n",
    " \n",
    "# Selecting data for year 2001 and 2002\n",
    "data_2001 = wswscs_df.join(date_dim_df.alias(\"date_dim_2001\"), wswscs_df[\"D_WEEK_SEQ\"] == col(\"date_dim_2001.d_week_seq\")) \\\n",
    "    .where(col(\"date_dim_2001.d_year\") == 2001) \\\n",
    "    .selectExpr(\"date_dim_2001.d_week_seq as d_week_seq1\", \"sun_sales as sun_sales1\", \"mon_sales as mon_sales1\",\n",
    "                \"tue_sales as tue_sales1\", \"wed_sales as wed_sales1\", \"thu_sales as thu_sales1\",\n",
    "                \"fri_sales as fri_sales1\", \"sat_sales as sat_sales1\")\n",
    " \n",
    "data_2002 = wswscs_df.join(date_dim_df.alias(\"date_dim_2002\"), wswscs_df[\"D_WEEK_SEQ\"] == col(\"date_dim_2002.d_week_seq\")) \\\n",
    "    .where(col(\"date_dim_2002.d_year\") == 2002) \\\n",
    "    .selectExpr(\"date_dim_2002.d_week_seq as d_week_seq2\", \"sun_sales as sun_sales2\", \"mon_sales as mon_sales2\",\n",
    "                \"tue_sales as tue_sales2\", \"wed_sales as wed_sales2\", \"thu_sales as thu_sales2\",\n",
    "                \"fri_sales as fri_sales2\", \"sat_sales as sat_sales2\")\n",
    " \n",
    "# Joining data for the two years\n",
    "final_data = data_2001.join(data_2002, (data_2001[\"d_week_seq1\"] == data_2002[\"d_week_seq2\"] - 53)) \\\n",
    "    .orderBy(\"d_week_seq1\") \\\n",
    "    .selectExpr(\"d_week_seq1\",\n",
    "                \"round(sun_sales1/sun_sales2, 2) as sun_sales_ratio\",\n",
    "                \"round(mon_sales1/mon_sales2, 2) as mon_sales_ratio\",\n",
    "                \"round(tue_sales1/tue_sales2, 2) as tue_sales_ratio\",\n",
    "                \"round(wed_sales1/wed_sales2, 2) as wed_sales_ratio\",\n",
    "                \"round(thu_sales1/thu_sales2, 2) as thu_sales_ratio\",\n",
    "                \"round(fri_sales1/fri_sales2, 2) as fri_sales_ratio\",\n",
    "                \"round(sat_sales1/sat_sales2, 2) as sat_sales_ratio\")\n",
    " \n",
    "# Displaying the final result\n",
    "final_data.show()\n",
    "total_count = final_data.select(count(\"d_week_seq1\"))\n",
    "total_count.show()\n",
    "\n",
    "filtered_df = final_data.filter(final_data[\"d_week_seq1\"] == 5270)\n",
    "\n",
    "# Show filtered DataFrame\n",
    "filtered_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3de7d52-02c3-4ed3-ab76-724acb3e661c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+----------+\n|d_year|i_brand_id|             i_brand|   sum_agg|\n+------+----------+--------------------+----------+\n|  1998|   8015003|      scholarmaxi #3|8109206.01|\n|  1998|   2001002|     amalgimporto #2|8097233.52|\n|  1998|  10006017|   corpunivamalg #17|8087166.91|\n|  1998|   6012005|     importobrand #5|8084274.20|\n|  1998|  10003017|exportiunivamalg #17|8082571.61|\n|  1998|   7014006| edu packnameless #6|8077215.29|\n|  1998|   6007002|        brandcorp #2|8070730.80|\n|  1998|   8013003|      exportimaxi #3|8067421.37|\n|  1998|   7014007| edu packnameless #7|8063499.38|\n|  1998|  10009013|   maxiunivamalg #13|8054987.47|\n|  1998|  10016003|   corpamalgamalg #3|8047113.41|\n|  1998|  10011006|  amalgamalgamalg #6|8045093.69|\n|  1998|   8003010| exportinameless #10|8041132.21|\n|  1998|   8005003|  scholarnameless #3|8035518.75|\n|  1998|  10008002|namelessunivamalg #2|8035242.64|\n|  1998|   8007001|    brandnameless #1|8029976.18|\n|  1998|   9005009|      scholarmaxi #9|8020260.31|\n|  1998|   8006005|     corpnameless #5|8015254.22|\n|  1998|   9010008|    univunivamalg #8|7990015.67|\n|  1998|   6009004|         maxicorp #4|7987892.36|\n+------+----------+--------------------+----------+\nonly showing top 20 rows\n\n+--------------+\n|  sum(sum_agg)|\n+--------------+\n|15438235538.22|\n+--------------+\n\n+--------------+\n|count(sum_agg)|\n+--------------+\n|          1186|\n+--------------+\n\n+------+----------+------------------+------------+\n|d_year|i_brand_id|           i_brand|     sum_agg|\n+------+----------+------------------+------------+\n|  2000|   3004001|edu packexporti #1| 50234477.24|\n|  2002|   3004001|     amalgamalg #2|  8861332.12|\n|  2001|   3004001|     amalgamalg #2|  8307683.68|\n|  2002|   3004001|   amalgimporto #2|  8297730.96|\n|  1999|   3004001|edu packexporti #1|104903234.23|\n|  1998|   3004001|edu packexporti #1|103181457.14|\n|  2001|   3004001|   amalgimporto #2|  8391901.82|\n|  2002|   3004001|edu packexporti #1|  8489142.58|\n+------+----------+------------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# query: 3\n",
    "\n",
    "sfOptions = {\n",
    "  \"sfURL\" : \"https://uh98701.ap-southeast-1.snowflakecomputing.com\",\n",
    "  \"sfAccount\" : \"uh98701.ap-southeast-1\",\n",
    "  \"sfUser\" : \"UMESHCHANDRAKADEM\",\n",
    "  \"sfPassword\" : \"Umesh.K@127\",\n",
    "  \"sfDatabase\" : \"SNOWFLAKE_SAMPLE_DATA\",\n",
    "  \"sfSchema\" : \"TPCDS_SF10TCL\",\n",
    "  \"sfWarehouse\" : \"COMPUTE_WH\"\n",
    "}\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum as spark_sum\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Convert SQL to PySpark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read tables as DataFrames\n",
    "date_dim_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"date_dim\").load()\n",
    "store_sales_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"store_sales\").load()\n",
    "item_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"item\").load()\n",
    "\n",
    "# Join DataFrames and apply filters\n",
    "joined_df = date_dim_df.join(store_sales_df, date_dim_df[\"d_date_sk\"] == store_sales_df[\"ss_sold_date_sk\"]) \\\n",
    "    .join(item_df, store_sales_df[\"ss_item_sk\"] == item_df[\"i_item_sk\"]) \\\n",
    "    .where((date_dim_df[\"d_moy\"] == 11) & (item_df[\"i_manufact_id\"] == 128))\n",
    "\n",
    "# Perform aggregation\n",
    "agg_df = joined_df.groupBy(date_dim_df[\"d_year\"], item_df[\"i_brand_id\"], item_df[\"i_brand\"]) \\\n",
    "    .agg(spark_sum(\"ss_ext_sales_price\").alias(\"sum_agg\"))\n",
    "\n",
    "# Order by specified columns\n",
    "ordered_df = agg_df.orderBy(\"d_year\", \"sum_agg\", \"i_brand_id\", ascending=[True, False, True])\n",
    "\n",
    "# Show DataFrame\n",
    "ordered_df.show()\n",
    "# sum of all records\n",
    "sum_df = ordered_df.select(sum('sum_agg'))\n",
    "\n",
    "# Show DataFrame\n",
    "sum_df.show()\n",
    "\n",
    "# count of records\n",
    "total_count = ordered_df.select(count(\"sum_agg\"))\n",
    "total_count.show()\n",
    "\n",
    "# filter of record\n",
    "filtered_df = ordered_df.filter(ordered_df[\"i_brand_id\"] == 3004001)\n",
    "\n",
    "# Show filtered DataFrame\n",
    "filtered_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68dfcbee-aaf4-423f-b714-afda8f511f74",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------------+------------------+----------------------+----------------------------+\n|     customer_id|customer_first_name|customer_last_name|customer_birth_country|customer_preferred_cust_flag|\n+----------------+-------------------+------------------+----------------------+----------------------------+\n|AAAAAAAAJBLMOCAA|            William|          Chandler|                MONACO|                           Y|\n|AAAAAAAAADFDLGDA|            Brandon|         Dominguez|               ALBANIA|                           Y|\n|AAAAAAAADGOJEFDA|             Samual|             Adams|              CAMBODIA|                           N|\n|AAAAAAAAFCJAGHCA|                Lee|              null|                  null|                        null|\n|AAAAAAAANBDJMADA|             Bonnie|           Shelton|         FRENCH GUIANA|                           Y|\n|AAAAAAAADNGALCBA|               Nona|              null|                GREECE|                           Y|\n|AAAAAAAAJIPIMNDA|            William|         Pritchett|                 GHANA|                           Y|\n|AAAAAAAAOGDHJABA|           Mellissa|           Mallory|            AZERBAIJAN|                           Y|\n|AAAAAAAACDKNEIAA|             Robert|           Thomson|                  GUAM|                           Y|\n|AAAAAAAABPNPNDCA|             Pamela|           Mendoza|               BELARUS|                           Y|\n|AAAAAAAAKLKFKHCA|            Deborah|          Friedman|               ALGERIA|                           N|\n|AAAAAAAAAHFCBLBA|               Nina|           Gabriel|                  TOGO|                           Y|\n|AAAAAAAAEEFMJBBA|              Betsy|           Johnson|                TUVALU|                           N|\n|AAAAAAAAJOACLKAA|              Jesus|              Land|               BAHAMAS|                           N|\n|AAAAAAAAMALDNHDA|              Laura|            Norman|                RWANDA|                           N|\n|AAAAAAAAKCEEOMAA|              Brian|               Cox|             VENEZUELA|                           N|\n|AAAAAAAACGPJAIBA|              Irene|            Warren|               LESOTHO|                           Y|\n|AAAAAAAAGECBJNCA|            Michael|             Beard|                  null|                        null|\n|AAAAAAAADKBELACA|              Peter|             Avila|                 ARUBA|                           Y|\n|AAAAAAAAKLMILDDA|              Terry|          Robinson|  SYRIAN ARAB REPUBLIC|                           N|\n+----------------+-------------------+------------------+----------------------+----------------------------+\nonly showing top 20 rows\n\ncustomer_id count:\n+------------------+\n|count(customer_id)|\n+------------------+\n|               697|\n+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#query: 4\n",
    "sfOptions = {\n",
    "  \"sfURL\" : \"https://uh98701.ap-southeast-1.snowflakecomputing.com\",\n",
    "  \"sfAccount\" : \"uh98701.ap-southeast-1\",\n",
    "  \"sfUser\" : \"UMESHCHANDRAKADEM\",\n",
    "  \"sfPassword\" : \"Umesh.K@127\",\n",
    "  \"sfDatabase\" : \"SNOWFLAKE_SAMPLE_DATA\",\n",
    "  \"sfSchema\" : \"TPCDS_SF10TCL\",\n",
    "  \"sfWarehouse\" : \"COMPUTE_WH\"\n",
    "}\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, col, when, count\n",
    " \n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Customer Spending Analysis\") \\\n",
    "    .getOrCreate()\n",
    " \n",
    "# Read data from Snowflake into Spark DataFrame\n",
    "customer_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"customer\").load()\n",
    "store_sales_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"store_sales\").load()\n",
    "catalog_sales_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"catalog_sales\").load()\n",
    " \n",
    "# Define the qualification substitution parameters\n",
    "YEAR = 2001\n",
    " \n",
    "# Calculate year total for store sales\n",
    "from pyspark.sql.functions import sum, col, lit\n",
    " \n",
    "# Calculate year total for store sales\n",
    "store_sales_year_total = store_sales_df.groupBy(\"ss_customer_sk\").agg(\n",
    "    sum(((store_sales_df[\"ss_ext_list_price\"] - store_sales_df[\"ss_ext_wholesale_cost\"] - store_sales_df[\"ss_ext_discount_amt\"]) + store_sales_df[\"ss_ext_sales_price\"]) / 2).alias(\"year_total\")\n",
    ").withColumn(\"sale_type\", lit(\"s\"))\n",
    " \n",
    "# Calculate year total for catalog sales\n",
    "catalog_sales_year_total = catalog_sales_df.groupBy(\"cs_bill_customer_sk\").agg(\n",
    "    sum((((catalog_sales_df[\"cs_ext_list_price\"] - catalog_sales_df[\"cs_ext_wholesale_cost\"] - catalog_sales_df[\"cs_ext_discount_amt\"]) + catalog_sales_df[\"cs_ext_sales_price\"]) / 2)).alias(\"year_total\")\n",
    ").withColumn(\"sale_type\", lit(\"c\"))\n",
    " \n",
    " \n",
    "# Calculate year total for catalog sales\n",
    "catalog_sales_year_total = catalog_sales_df.groupBy(\"cs_bill_customer_sk\").agg(\n",
    "    sum((((catalog_sales_df[\"cs_ext_list_price\"] - catalog_sales_df[\"cs_ext_wholesale_cost\"] - catalog_sales_df[\"cs_ext_discount_amt\"]) + catalog_sales_df[\"cs_ext_sales_price\"]) / 2)).alias(\"year_total\")\n",
    ").withColumn(\"sale_type\", lit(\"c\"))\n",
    " \n",
    "# Find customers who spent more via catalog than in stores\n",
    "preferred_customers = store_sales_year_total.join(catalog_sales_year_total, store_sales_year_total[\"ss_customer_sk\"] == catalog_sales_year_total[\"cs_bill_customer_sk\"], \"inner\") \\\n",
    "    .join(customer_df, store_sales_year_total[\"ss_customer_sk\"] == customer_df[\"c_customer_sk\"], \"inner\") \\\n",
    "    .filter((store_sales_year_total[\"year_total\"] > 0) & (catalog_sales_year_total[\"year_total\"] > 0) & \\\n",
    "            (catalog_sales_year_total[\"year_total\"] / store_sales_year_total[\"year_total\"] > 1)) \\\n",
    "    .select(\n",
    "        customer_df[\"c_customer_id\"].alias(\"customer_id\"),\n",
    "        customer_df[\"c_first_name\"].alias(\"customer_first_name\"),\n",
    "        customer_df[\"c_last_name\"].alias(\"customer_last_name\"),\n",
    "        customer_df[\"c_birth_country\"].alias(\"customer_birth_country\"),\n",
    "        customer_df[\"c_preferred_cust_flag\"].alias(\"customer_preferred_cust_flag\")\n",
    "    )\n",
    " \n",
    "# Show the result\n",
    "preferred_customers.show()\n",
    "print('customer_id count:')\n",
    "total_count = preferred_customers.select(count(\"customer_id\"))\n",
    "total_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ee2e1f9-c876-42b7-87e6-ba458a115cc6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5. query\n",
    "\n",
    "sfOptions = {\n",
    "  \"sfURL\" : \"https://uh98701.ap-southeast-1.snowflakecomputing.com\",\n",
    "  \"sfAccount\" : \"uh98701.ap-southeast-1\",\n",
    "  \"sfUser\" : \"UMESHCHANDRAKADEM\",\n",
    "  \"sfPassword\" : \"Umesh.K@127\",\n",
    "  \"sfDatabase\" : \"SNOWFLAKE_SAMPLE_DATA\",\n",
    "  \"sfSchema\" : \"TPCDS_SF10TCL\",\n",
    "  \"sfWarehouse\" : \"COMPUTE_WH\"\n",
    "}\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum as spark_sum, col, when, lit, date_add\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Convert SQL to PySpark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read tables as DataFrames\n",
    "store_sales_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"store_sales\").load()\n",
    "store_returns_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"store_returns\").load()\n",
    "catalog_sales_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"catalog_sales\").load()\n",
    "catalog_returns_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"catalog_returns\").load()\n",
    "web_sales_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"web_sales\").load()\n",
    "web_returns_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"web_returns\").load()\n",
    "store_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"store\").load()\n",
    "catalog_page_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"catalog_page\").load()\n",
    "web_site_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"web_site\").load()\n",
    "date_dim_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"date_dim\").load()\n",
    "\n",
    "# Create temporary views for tables\n",
    "store_sales_df.createOrReplaceTempView(\"store_sales\")\n",
    "store_returns_df.createOrReplaceTempView(\"store_returns\")\n",
    "catalog_sales_df.createOrReplaceTempView(\"catalog_sales\")\n",
    "catalog_returns_df.createOrReplaceTempView(\"catalog_returns\")\n",
    "web_sales_df.createOrReplaceTempView(\"web_sales\")\n",
    "web_returns_df.createOrReplaceTempView(\"web_returns\")\n",
    "store_df.createOrReplaceTempView(\"store\")\n",
    "catalog_page_df.createOrReplaceTempView(\"catalog_page\")\n",
    "web_site_df.createOrReplaceTempView(\"web_site\")\n",
    "date_dim_df.createOrReplaceTempView(\"date_dim\")\n",
    "\n",
    "# Execute the PySpark code\n",
    "result_df = store_sales_df.join(\n",
    "\n",
    "    date_dim_df,\n",
    "    store_sales_df[\"ss_sold_date_sk\"] == date_dim_df[\"d_date_sk\"]\n",
    ").join(\n",
    "    store_df,\n",
    "    store_sales_df[\"ss_store_sk\"] == store_df[\"s_store_sk\"]\n",
    ").where(\n",
    "    (col(\"d_date\").between(\"2000-08-23\", date_add(\"2000-08-23\", 14))) &\n",
    "    (col(\"store_sales_df\") == \"store\")\n",
    ").groupBy(\n",
    "    \"s_store_id\"\n",
    ").agg(\n",
    "    spark_sum(\"ss_sales_price\").alias(\"sales\"),\n",
    "    spark_sum(\"ss_net_profit\").alias(\"profit\"),\n",
    "    spark_sum(when(col(\"sr_return_amt\").isNull(), 0).otherwise(col(\"sr_return_amt\"))).alias(\"returns\"),\n",
    "    spark_sum(when(col(\"sr_net_loss\").isNull(), 0).otherwise(col(\"sr_net_loss\"))).alias(\"profit_loss\")\n",
    ").select(\n",
    "    col(\"channel\"),\n",
    "    col(\"id\"),\n",
    "    spark_sum(\"sales\").alias(\"sales\"),\n",
    "    spark_sum(\"returns\").alias(\"returns\"),\n",
    "    (spark_sum(\"profit\") - spark_sum(\"profit_loss\")).alias(\"profit\")\n",
    ").groupBy(\n",
    "    rollup(\"channel\", \"id\")\n",
    ").orderBy(\n",
    "    \"channel\",\n",
    "    \"id\"\n",
    ")\n",
    "\n",
    "# Show DataFrame\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8fb92ab-0ad7-4d5f-8ce8-d1ba280b3ffe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n|ca_state|    cnt|\n+--------+-------+\n|      DC|  88676|\n|      DE| 261918|\n|      HI| 353226|\n|      RI| 440450|\n|      CT| 707140|\n|      NH| 907516|\n|      MA|1234544|\n|      SC|4094370|\n|      ND|4688950|\n|      LA|4866379|\n|      WV|4885452|\n|      MT|4970962|\n|      CA|5160869|\n|      IN|8054158|\n|      NE|8263964|\n|      TN|8460637|\n|    null|8526207|\n|      IA|8788940|\n|      NC|8895997|\n|      IL|8953018|\n+--------+-------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#6.query\n",
    "\n",
    "sfOptions = {\n",
    "  \"sfURL\" : \"https://uh98701.ap-southeast-1.snowflakecomputing.com\",\n",
    "  \"sfAccount\" : \"uh98701.ap-southeast-1\",\n",
    "  \"sfUser\" : \"UMESHCHANDRAKADEM\",\n",
    "  \"sfPassword\" : \"Umesh.K@127\",\n",
    "  \"sfDatabase\" : \"SNOWFLAKE_SAMPLE_DATA\",\n",
    "  \"sfSchema\" : \"TPCDS_SF10TCL\",\n",
    "  \"sfWarehouse\" : \"COMPUTE_WH\"\n",
    "}\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, count\n",
    " \n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Convert SQL to PySpark\") \\\n",
    "    .getOrCreate()\n",
    " \n",
    "# Read necessary tables\n",
    "customer_address_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"customer_address\").load()\n",
    "customer_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"customer\").load()\n",
    "store_sales_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"store_sales\").load()\n",
    "date_dim_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"date_dim\").load()\n",
    "item_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"item\").load()\n",
    " \n",
    "# Filter date_dim for the specified year and month\n",
    "date_dim_filtered_df = date_dim_df.filter((col(\"d_year\") == 2001) & (col(\"d_moy\") == 1))\n",
    " \n",
    "# Join the tables and perform necessary operations\n",
    "result_df = customer_address_df.alias(\"a\") \\\n",
    "    .join(customer_df.alias(\"c\"), col(\"a.ca_address_sk\") == col(\"c.c_current_addr_sk\")) \\\n",
    "    .join(store_sales_df.alias(\"s\"), col(\"c.c_customer_sk\") == col(\"s.ss_customer_sk\")) \\\n",
    "    .join(date_dim_filtered_df.alias(\"d\"), col(\"s.ss_sold_date_sk\") == col(\"d.d_date_sk\")) \\\n",
    "    .join(item_df.alias(\"i\"), col(\"s.ss_item_sk\") == col(\"i.i_item_sk\")) \\\n",
    "    .groupBy(\"a.ca_state\") \\\n",
    "    .agg({\"*\": \"count\"}) \\\n",
    "    .withColumnRenamed(\"count(1)\", \"cnt\") \\\n",
    "    .filter(col(\"cnt\") >= 10) \\\n",
    "    .orderBy(\"cnt\", \"a.ca_state\")\n",
    " \n",
    "# Show the result\n",
    "result_df.show()\n",
    "\n",
    "total_count = result_df.select(count(\"sum_agg\"))\n",
    "total_count.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82753bc2-132c-4812-8d03-6cf85304a7fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+---------+----------+---------+\n|       i_item_id|     agg1|     agg2|      agg3|     agg4|\n+----------------+---------+---------+----------+---------+\n|AAAAAAAAHPBJFAAA|50.200000|74.191231|223.677914|37.163957|\n|AAAAAAAAHPBKBAAA|44.585784|71.127878|139.085196|41.612083|\n|AAAAAAAAHPBKEAAA|50.542320|78.371781|218.257406|40.271223|\n|AAAAAAAAHPBLAAAA|47.623306|68.934278|100.518747|33.447711|\n|AAAAAAAAHPBLDAAA|49.862069|73.515252|146.751379|36.030849|\n|AAAAAAAAHPBMCAAA|52.853583|75.399875|182.279190|42.183323|\n|AAAAAAAAHPBMFAAA|51.182927|81.978927|169.831127|40.875951|\n|AAAAAAAAHPBNBAAA|46.204188|82.177192|200.035354|42.832068|\n|AAAAAAAAHPBNEAAA|53.287834|78.194911|266.799021|38.547567|\n|AAAAAAAAHPBOAAAA|53.097619|83.946000|107.135976|38.936524|\n|AAAAAAAAHPBODAAA|55.334975|70.992868|130.340983|32.699828|\n|AAAAAAAAHPBPCAAA|46.836735|79.985587|169.134987|33.586412|\n|AAAAAAAAHPBPFAAA|49.193460|72.728747|168.208184|36.687398|\n|AAAAAAAAHPCABAAA|56.095142|73.083144|151.994949|36.038337|\n|AAAAAAAAHPCAEAAA|46.290030|80.862219|179.133788|36.768127|\n|AAAAAAAAHPCBAAAA|51.332326|74.805136|216.849940|38.430606|\n|AAAAAAAAHPCBDAAA|49.370629|77.010187|294.646075|32.603543|\n|AAAAAAAAHPCBGAAA|43.915000|77.628450|185.245614|41.872850|\n|AAAAAAAAHPCCCAAA|50.458791|72.739093|168.239835|39.951405|\n|AAAAAAAAHPCCFAAA|48.283912|72.639143|173.278608|33.874304|\n+----------------+---------+---------+----------+---------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#query: 7\n",
    "\n",
    "sfOptions = {\n",
    "  \"sfURL\" : \"https://uh98701.ap-southeast-1.snowflakecomputing.com\",\n",
    "  \"sfAccount\" : \"uh98701.ap-southeast-1\",\n",
    "  \"sfUser\" : \"UMESHCHANDRAKADEM\",\n",
    "  \"sfPassword\" : \"Umesh.K@127\",\n",
    "  \"sfDatabase\" : \"SNOWFLAKE_SAMPLE_DATA\",\n",
    "  \"sfSchema\" : \"TPCDS_SF10TCL\",\n",
    "  \"sfWarehouse\" : \"COMPUTE_WH\"\n",
    "}\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col\n",
    " \n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Convert SQL to PySpark\") \\\n",
    "    .getOrCreate()\n",
    " \n",
    "# Read necessary tables\n",
    "store_sales_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"store_sales\").load()\n",
    "customer_demographics_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"customer_demographics\").load()\n",
    "date_dim_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"date_dim\").load()\n",
    "item_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"item\").load()\n",
    "promotion_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"promotion\").load()\n",
    " \n",
    "# Filter date_dim for the specified year\n",
    "date_dim_filtered_df = date_dim_df.filter(col(\"d_year\") == 2000)\n",
    " \n",
    "# Join the tables and perform necessary operations\n",
    "result_df = store_sales_df.alias(\"ss\") \\\n",
    "    .join(customer_demographics_df.alias(\"cd\"), col(\"ss.ss_cdemo_sk\") == col(\"cd.cd_demo_sk\")) \\\n",
    "    .join(date_dim_filtered_df.alias(\"d\"), col(\"ss.ss_sold_date_sk\") == col(\"d.d_date_sk\")) \\\n",
    "    .join(item_df.alias(\"i\"), col(\"ss.ss_item_sk\") == col(\"i.i_item_sk\")) \\\n",
    "    .join(promotion_df.alias(\"p\"), col(\"ss.ss_promo_sk\") == col(\"p.p_promo_sk\")) \\\n",
    "    .filter((col(\"cd.cd_gender\") == \"M\") &\n",
    "            (col(\"cd.cd_marital_status\") == \"S\") &\n",
    "            (col(\"cd.cd_education_status\") == \"College\") &\n",
    "            ((col(\"p.p_channel_email\") == \"N\") | (col(\"p.p_channel_event\") == \"N\"))) \\\n",
    "    .groupBy(\"i.i_item_id\") \\\n",
    "    .agg(avg(\"ss.ss_quantity\").alias(\"agg1\"),\n",
    "         avg(\"ss.ss_list_price\").alias(\"agg2\"),\n",
    "         avg(\"ss.ss_coupon_amt\").alias(\"agg3\"),\n",
    "         avg(\"ss.ss_sales_price\").alias(\"agg4\")) \\\n",
    "    .orderBy(\"i.i_item_id\")\n",
    " \n",
    "# Show the result\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78b9f6ee-1b24-440c-9178-88ea1f7b9ffb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# query: 8\n",
    "\n",
    "sfOptions = {\n",
    "  \"sfURL\" : \"https://uh98701.ap-southeast-1.snowflakecomputing.com\",\n",
    "  \"sfAccount\" : \"uh98701.ap-southeast-1\",\n",
    "  \"sfUser\" : \"UMESHCHANDRAKADEM\",\n",
    "  \"sfPassword\" : \"Umesh.K@127\",\n",
    "  \"sfDatabase\" : \"SNOWFLAKE_SAMPLE_DATA\",\n",
    "  \"sfSchema\" : \"TPCDS_SF10TCL\",\n",
    "  \"sfWarehouse\" : \"COMPUTE_WH\"\n",
    "}\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, substring, sum as _sum\n",
    " \n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    ".appName(\"Snowflake to Spark\") \\\n",
    ".getOrCreate()\n",
    " \n",
    "# Read data from Snowflake into Spark DataFrames\n",
    "store_sales_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"store_sales\").load()\n",
    "date_dim_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"date_dim\").load()\n",
    "store_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"store\").load()\n",
    "customer_address_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"customer_address\").load()\n",
    "customer_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"customer\").load()\n",
    " \n",
    "# Subquery to filter the zip codes\n",
    "valid_zips_df = customer_address_df.select(substring(\"CA_ZIP\", 1, 5).alias(\"CA_ZIP\")) \\\n",
    ".filter(col(\"CA_ZIP\").isin(\n",
    "'67436', '26121', '38443', '63157', '68856', '19485', '86425', '26741',\n",
    "'70991', '60899', '63573', '47556', '56193', '93314', '87827', '62017',\n",
    "'85067', '95390', '48091', '10261', '81845', '41790', '42853', '24675',\n",
    "'12840', '60065', '84430', '57451', '24021', '91735', '75335', '71935',\n",
    "'34482', '56943', '70695', '52147', '56251', '28411', '86653', '23005',\n",
    "'22478', '29031', '34398', '15365', '42460', '33337', '59433', '73943',\n",
    "'72477', '74081', '74430', '64605', '39006', '11226', '49057', '97308',\n",
    "'42663', '18187', '19768', '43454', '32147', '76637', '51975', '11181',\n",
    "'45630', '33129', '45995', '64386', '55522', '26697', '20963', '35154',\n",
    "'64587', '49752', '66386', '30586', '59286', '13177', '66646', '84195',\n",
    "'74316', '36853', '32927', '12469', '11904', '36269', '17724', '55346',\n",
    "'12595', '53988', '65439', '28015', '63268', '73590', '29216', '82575',\n",
    "'69267', '13805', '91678', '79460', '94152', '14961', '15419', '48277',\n",
    "'62588', '55493', '28360', '14152', '55225', '18007', '53705', '56573',\n",
    "'80245', '71769', '57348', '36845', '13039', '17270', '22363', '83474',\n",
    "'25294', '43269', '77666', '15488', '99146', '64441', '43338', '38736',\n",
    "'62754', '48556', '86057', '23090', '38114', '66061', '18910', '84385',\n",
    "'23600', '19975', '27883', '65719', '19933', '32085', '49731', '40473',\n",
    "'27190', '46192', '23949', '44738', '12436', '64794', '68741', '15333',\n",
    "'24282', '49085', '31844', '71156', '48441', '17100', '98207', '44982',\n",
    "'20277', '71496', '96299', '37583', '22206', '89174', '30589', '61924',\n",
    "'53079', '10976', '13104', '42794', '54772', '15809', '56434', '39975',\n",
    "'13874', '30753', '77598', '78229', '59478', '12345', '55547', '57422',\n",
    "'42600', '79444', '29074', '29752', '21676', '32096', '43044', '39383',\n",
    "'37296', '36295', '63077', '16572', '31275', '18701', '40197', '48242',\n",
    "'27219', '49865', '84175', '30446', '25165', '13807', '72142', '70499',\n",
    "'70464', '71429', '18111', '70857', '29545', '36425', '52706', '36194',\n",
    "'42963', '75068', '47921', '74763', '90990', '89456', '62073', '88397',\n",
    "'73963', '75885', '62657', '12530', '81146', '57434', '25099', '41429',\n",
    "'98441', '48713', '52552', '31667', '14072', '13903', '44709', '85429',\n",
    "'58017', '38295', '44875', '73541', '30091', '12707', '23762', '62258',\n",
    "'33247', '78722', '77431', '14510', '35656', '72428', '92082', '35267',\n",
    "'43759', '24354', '90952', '11512', '21242', '22579', '56114', '32339',\n",
    "'52282', '41791', '24484', '95020', '28408', '99710', '11899', '43344',\n",
    "'72915', '27644', '62708', '74479', '17177', '32619', '12351', '91339',\n",
    "'31169', '57081', '53522', '16712', '34419', '71779', '44187', '46206',\n",
    "'96099', '61910', '53664', '12295', '31837', '33096', '10813', '63048',\n",
    "'31732', '79118', '73084', '72783', '84952', '46965', '77956', '39815',\n",
    "'32311', '75329', '48156', '30826', '49661', '13736', '92076', '74865',\n",
    "'88149', '92397', '52777', '68453', '32012', '21222', '52721', '24626',\n",
    "'18210', '42177', '91791', '75251', '82075', '44372', '45542', '20609',\n",
    "'60115', '17362', '22750', '90434', '31852', '54071', '33762', '14705',\n",
    "'40718', '56433', '30996', '40657', '49056', '23585', '66455', '41021',\n",
    "'74736', '72151', '37007', '21729', '60177', '84558', '59027', '93855',\n",
    "'60022', '86443', '19541', '86886', '30532', '39062', '48532', '34713',\n",
    "'52077', '22564', '64638', '15273', '31677', '36138', '62367', '60261',\n",
    "'80213', '42818', '25113', '72378', '69802', '69096', '55443', '28820',\n",
    "'13848', '78258', '37490', '30556', '77380', '28447', '44550', '26791',\n",
    "'70609', '82182', '33306', '43224', '22322', '86959', '68519', '14308',\n",
    "'46501', '81131', '34056', '61991', '19896', '87804', '65774', '92564'\n",
    "))\n",
    " \n",
    "# Second subquery to filter zip codes with preferred customers\n",
    "preferred_cust_zips_df = customer_df.filter(col(\"C_CURRENT_HH_DEMOGRAPHIC\") == \"preferred\") \\\n",
    ".join(customer_address_df, customer_df.C_CURRENT_ADDR_SK == customer_address_df.CA_ADDRESS_SK) \\\n",
    ".select(substring(customer_address_df.CA_ZIP, 1, 5).alias(\"CA_ZIP\"))\n",
    " \n",
    "# Perform the intersection\n",
    "common_zips_df = valid_zips_df.select(\"CA_ZIP\").intersect(preferred_cust_zips_df.select(\"CA_ZIP\"))\n",
    " \n",
    "# Join the DataFrames to filter sales by these zip codes\n",
    "result_df = store_sales_df.join(common_zips_df, store_sales_df.SS_ADDR_SK == customer_address_df.CA_ADDRESS_SK) \\\n",
    ".join(date_dim_df, store_sales_df.SS_SOLD_DATE_SK == date_dim_df.D_DATE_SK) \\\n",
    ".join(store_df, store_sales_df.SS_STORE_SK == store_df.S_STORE_SK) \\\n",
    ".groupBy(store_df.S_STORE_NAME) \\\n",
    ".agg(_sum(store_sales_df.SS_SALES_PRICE).alias(\"TOTAL_SALES\"))\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58a77bbb-d039-4261-b8ee-d5ae7ea63bef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|R_REASON_SK|     R_REASON_ID|      R_REASON_DESC|             bucket1|             bucket2|             bucket3|             bucket4|             bucket5|\n+-----------+----------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|          1|AAAAAAAABAAAAAAA|Package was damaged|795.2997990000000...|2310.257528000000...|3824.761249000000...|5340.264830000000...|6855.546035000000...|\n+-----------+----------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# query: 9\n",
    "\n",
    "sfOptions = {\n",
    "  \"sfURL\" : \"https://uh98701.ap-southeast-1.snowflakecomputing.com\",\n",
    "  \"sfAccount\" : \"uh98701.ap-southeast-1\",\n",
    "  \"sfUser\" : \"UMESHCHANDRAKADEM\",\n",
    "  \"sfPassword\" : \"Umesh.K@127\",\n",
    "  \"sfDatabase\" : \"SNOWFLAKE_SAMPLE_DATA\",\n",
    "  \"sfSchema\" : \"TPCDS_SF10TCL\",\n",
    "  \"sfWarehouse\" : \"COMPUTE_WH\"\n",
    "}\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import col, avg, count, when\n",
    " \n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    ".appName(\"SQL to PySpark Conversion\") \\\n",
    ".getOrCreate()\n",
    " \n",
    "# Read data from Snowflake into Spark DataFrames\n",
    "store_sales_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"store_sales\").load()\n",
    "reason_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"reason\").load()\n",
    " \n",
    "# Function to calculate the metric based on the condition\n",
    "def calculate_metric(df, lower, upper, threshold):\n",
    "\n",
    "  count_value = df.filter((col(\"ss_quantity\") >= lower) & (col(\"ss_quantity\") <= upper)).count()\n",
    "  if count_value > threshold:\n",
    "    return df.filter((col(\"ss_quantity\") >= lower) & (col(\"ss_quantity\") <= upper)).agg(avg(\"ss_ext_list_price\")).collect()[0][0]\n",
    "  else:\n",
    "    return df.filter((col(\"ss_quantity\") >= lower) & (col(\"ss_quantity\") <= upper)).agg(avg(\"ss_net_profit\")).collect()[0][0]\n",
    " \n",
    "# Calculate each bucket value\n",
    "bucket1 = calculate_metric(store_sales_df, 1, 20, 3672)\n",
    "bucket2 = calculate_metric(store_sales_df, 21, 40, 3392)\n",
    "bucket3 = calculate_metric(store_sales_df, 41, 60, 32784)\n",
    "bucket4 = calculate_metric(store_sales_df, 61, 80, 26032)\n",
    "bucket5 = calculate_metric(store_sales_df, 81, 100, 23982)\n",
    " \n",
    "# Create a DataFrame with the calculated bucket values\n",
    "bucket_values = [(bucket1, bucket2, bucket3, bucket4, bucket5)]\n",
    "buckets_df = spark.createDataFrame(bucket_values, [\"bucket1\", \"bucket2\", \"bucket3\", \"bucket4\", \"bucket5\"])\n",
    " \n",
    "# Filter the reason table and join with the buckets DataFrame\n",
    "result_df = reason_df.filter(col(\"r_reason_sk\") == 1).crossJoin(buckets_df)\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c56b8f0-76b8-47e8-9fff-31d5e1b198a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+-------------------+--------------------+----------------+------------+---------------------+--------------------+----+----+----+----+----+----+\n|cd_gender|cd_marital_status|cd_education_status|cd_purchase_estimate|cd_credit_rating|cd_dep_count|cd_dep_employed_count|cd_dep_college_count|cnt1|cnt2|cnt3|cnt4|cnt5|cnt6|\n+---------+-----------------+-------------------+--------------------+----------------+------------+---------------------+--------------------+----+----+----+----+----+----+\n|        F|                M|    Advanced Degree|                 500|        Low Risk|           5|                    4|                   4|   1|   1|   1|   1|   1|   1|\n|        F|                M|    Advanced Degree|                 500|        Low Risk|           5|                    4|                   5|   1|   1|   1|   1|   1|   1|\n|        F|                M|    Advanced Degree|                 500|        Low Risk|           5|                    5|                   2|   2|   2|   2|   2|   2|   2|\n|        F|                M|    Advanced Degree|                 500|        Low Risk|           5|                    5|                   3|   1|   1|   1|   1|   1|   1|\n|        F|                M|    Advanced Degree|                 500|        Low Risk|           5|                    5|                   5|   2|   2|   2|   2|   2|   2|\n|        F|                M|    Advanced Degree|                 500|        Low Risk|           5|                    6|                   1|   1|   1|   1|   1|   1|   1|\n|        F|                M|    Advanced Degree|                 500|        Low Risk|           5|                    6|                   2|   1|   1|   1|   1|   1|   1|\n|        F|                M|    Advanced Degree|                 500|        Low Risk|           5|                    6|                   3|   1|   1|   1|   1|   1|   1|\n|        F|                M|    Advanced Degree|                 500|        Low Risk|           5|                    6|                   4|   3|   3|   3|   3|   3|   3|\n|        F|                M|    Advanced Degree|                 500|        Low Risk|           6|                    0|                   1|   1|   1|   1|   1|   1|   1|\n|        F|                M|    Advanced Degree|                 500|        Low Risk|           6|                    0|                   2|   1|   1|   1|   1|   1|   1|\n|        F|                M|    Advanced Degree|                 500|        Low Risk|           6|                    1|                   2|   1|   1|   1|   1|   1|   1|\n|        F|                M|    Advanced Degree|                 500|        Low Risk|           6|                    1|                   5|   2|   2|   2|   2|   2|   2|\n|        F|                M|    Advanced Degree|                 500|        Low Risk|           6|                    1|                   6|   1|   1|   1|   1|   1|   1|\n|        F|                M|    Advanced Degree|                 500|        Low Risk|           6|                    2|                   2|   1|   1|   1|   1|   1|   1|\n|        F|                M|    Advanced Degree|                 500|        Low Risk|           6|                    2|                   5|   1|   1|   1|   1|   1|   1|\n|        F|                M|    Advanced Degree|                 500|        Low Risk|           6|                    3|                   2|   1|   1|   1|   1|   1|   1|\n|        F|                M|    Advanced Degree|                 500|        Low Risk|           6|                    3|                   4|   1|   1|   1|   1|   1|   1|\n|        F|                M|    Advanced Degree|                 500|        Low Risk|           6|                    4|                   5|   1|   1|   1|   1|   1|   1|\n|        F|                M|    Advanced Degree|                 500|        Low Risk|           6|                    4|                   6|   1|   1|   1|   1|   1|   1|\n+---------+-----------------+-------------------+--------------------+----------------+------------+---------------------+--------------------+----+----+----+----+----+----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# query: 10\n",
    "\n",
    "sfOptions = {\n",
    "  \"sfURL\" : \"https://uh98701.ap-southeast-1.snowflakecomputing.com\",\n",
    "  \"sfAccount\" : \"uh98701.ap-southeast-1\",\n",
    "  \"sfUser\" : \"UMESHCHANDRAKADEM\",\n",
    "  \"sfPassword\" : \"Umesh.K@127\",\n",
    "  \"sfDatabase\" : \"SNOWFLAKE_SAMPLE_DATA\",\n",
    "  \"sfSchema\" : \"TPCDS_SF10TCL\",\n",
    "  \"sfWarehouse\" : \"COMPUTE_WH\"\n",
    "}\n",
    "\n",
    "from pyspark.sql.functions import col, count\n",
    " \n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    ".appName(\"SQL to PySpark Conversion\") \\\n",
    ".getOrCreate()\n",
    " \n",
    "# Read data from Snowflake into Spark DataFrames\n",
    "customer_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"customer\").load()\n",
    "customer_address_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"customer_address\").load()\n",
    "customer_demographics_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"customer_demographics\").load()\n",
    "store_sales_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"store_sales\").load()\n",
    "web_sales_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"web_sales\").load()\n",
    "catalog_sales_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"catalog_sales\").load()\n",
    "date_dim_df = spark.read.format(\"snowflake\").options(**sfOptions).option(\"dbtable\", \"date_dim\").load()\n",
    " \n",
    "# Filter date_dim for the required year and months\n",
    "filtered_date_dim_df = date_dim_df.filter((col(\"d_year\") == 2002) & (col(\"d_moy\").between(4, 7)))\n",
    " \n",
    "# Filter store_sales, web_sales, and catalog_sales for the required date range\n",
    "filtered_store_sales_df = store_sales_df.join(filtered_date_dim_df, store_sales_df[\"ss_sold_date_sk\"] == filtered_date_dim_df[\"d_date_sk\"])\n",
    "filtered_web_sales_df = web_sales_df.join(filtered_date_dim_df, web_sales_df[\"ws_sold_date_sk\"] == filtered_date_dim_df[\"d_date_sk\"])\n",
    "filtered_catalog_sales_df = catalog_sales_df.join(filtered_date_dim_df, catalog_sales_df[\"cs_sold_date_sk\"] == filtered_date_dim_df[\"d_date_sk\"])\n",
    " \n",
    "# Filter customers based on store_sales, web_sales, and catalog_sales\n",
    "store_sales_customers = filtered_store_sales_df.select(\"ss_customer_sk\").distinct()\n",
    "web_sales_customers = filtered_web_sales_df.select(\"ws_bill_customer_sk\").distinct()\n",
    "catalog_sales_customers = filtered_catalog_sales_df.select(\"cs_ship_customer_sk\").distinct()\n",
    " \n",
    "# Union all customers\n",
    "all_customers = store_sales_customers.union(web_sales_customers).union(catalog_sales_customers).distinct()\n",
    " \n",
    "# Filter customer and customer_address tables\n",
    "filtered_customer_address_df = customer_address_df.filter(col(\"ca_county\").isin(\n",
    "'Lycoming County', 'Sheridan County', 'Kandiyohi County', 'Pike County', 'Greene County'\n",
    "))\n",
    " \n",
    "# Join customer with filtered customer_address\n",
    "customer_with_address = customer_df.join(filtered_customer_address_df, customer_df[\"c_current_addr_sk\"] == filtered_customer_address_df[\"ca_address_sk\"])\n",
    " \n",
    "# Filter customers who are in the sales data\n",
    "final_customers = customer_with_address.join(all_customers, customer_with_address[\"c_customer_sk\"] == all_customers[\"ss_customer_sk\"])\n",
    " \n",
    "# Join with customer_demographics\n",
    "final_data = final_customers.join(customer_demographics_df, customer_demographics_df[\"cd_demo_sk\"] == final_customers[\"c_current_cdemo_sk\"])\n",
    " \n",
    "# Group by required columns and aggregate\n",
    "result_df = final_data.groupBy(\n",
    "\"cd_gender\",\n",
    "\"cd_marital_status\",\n",
    "\"cd_education_status\",\n",
    "\"cd_purchase_estimate\",\n",
    "\"cd_credit_rating\",\n",
    "\"cd_dep_count\",\n",
    "\"cd_dep_employed_count\",\n",
    "\"cd_dep_college_count\"\n",
    ").agg(\n",
    "count(\"*\").alias(\"cnt1\"),\n",
    "count(\"*\").alias(\"cnt2\"),\n",
    "count(\"*\").alias(\"cnt3\"),\n",
    "count(\"*\").alias(\"cnt4\"),\n",
    "count(\"*\").alias(\"cnt5\"),\n",
    "count(\"*\").alias(\"cnt6\")\n",
    ").orderBy(\n",
    "\"cd_gender\",\n",
    "\"cd_marital_status\",\n",
    "\"cd_education_status\",\n",
    "\"cd_purchase_estimate\",\n",
    "\"cd_credit_rating\",\n",
    "\"cd_dep_count\",\n",
    "\"cd_dep_employed_count\",\n",
    "\"cd_dep_college_count\"\n",
    ").limit(100)\n",
    "# Show the result\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01d1b966-58fd-47b1-a629-195b44a4a8d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 6. query\n",
    "\n",
    "sfOptions = {\n",
    "  \"sfURL\" : \"https://uh98701.ap-southeast-1.snowflakecomputing.com\",\n",
    "  \"sfAccount\" : \"uh98701.ap-southeast-1\",\n",
    "  \"sfUser\" : \"UMESHCHANDRAKADEM\",\n",
    "  \"sfPassword\" : \"Umesh.K@127\",\n",
    "  \"sfDatabase\" : \"SNOWFLAKE_SAMPLE_DATA\",\n",
    "  \"sfSchema\" : \"TPCDS_SF10TCL\",\n",
    "  \"sfWarehouse\" : \"COMPUTE_WH\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09dd3bbc-3d63-429d-a396-c134b7e3d248",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1369089997198724>:22\u001B[0m\n",
       "\u001B[1;32m     17\u001B[0m spark \u001B[38;5;241m=\u001B[39m SparkSession\u001B[38;5;241m.\u001B[39mbuilder \\\n",
       "\u001B[1;32m     18\u001B[0m     \u001B[38;5;241m.\u001B[39mappName(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCustomer Sales Count\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m     19\u001B[0m     \u001B[38;5;241m.\u001B[39mgetOrCreate()\n",
       "\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# Read data from the appropriate sources and create DataFrames\u001B[39;00m\n",
       "\u001B[0;32m---> 22\u001B[0m customer_address_df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mtable(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcustomer_address\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mca_address_sk\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mca_state\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     23\u001B[0m customer_df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mtable(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcustomer\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mc_customer_sk\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mc_current_addr_sk\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     24\u001B[0m store_sales_df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mtable(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstore_sales\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mss_customer_sk\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mss_sold_date_sk\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mss_item_sk\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:473\u001B[0m, in \u001B[0;36mDataFrameReader.table\u001B[0;34m(self, tableName)\u001B[0m\n",
       "\u001B[1;32m    439\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtable\u001B[39m(\u001B[38;5;28mself\u001B[39m, tableName: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[1;32m    440\u001B[0m     \u001B[38;5;124;03m\"\"\"Returns the specified table as a :class:`DataFrame`.\u001B[39;00m\n",
       "\u001B[1;32m    441\u001B[0m \n",
       "\u001B[1;32m    442\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.4.0\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    471\u001B[0m \u001B[38;5;124;03m    >>> _ = spark.sql(\"DROP TABLE tblA\")\u001B[39;00m\n",
       "\u001B[1;32m    472\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m--> 473\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtable\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtableName\u001B[49m\u001B[43m)\u001B[49m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `customer_address` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
       "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
       "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.;\n",
       "'UnresolvedRelation [customer_address], [], false\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-1369089997198724>:22\u001B[0m\n\u001B[1;32m     17\u001B[0m spark \u001B[38;5;241m=\u001B[39m SparkSession\u001B[38;5;241m.\u001B[39mbuilder \\\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;241m.\u001B[39mappName(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCustomer Sales Count\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;241m.\u001B[39mgetOrCreate()\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# Read data from the appropriate sources and create DataFrames\u001B[39;00m\n\u001B[0;32m---> 22\u001B[0m customer_address_df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mtable(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcustomer_address\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mca_address_sk\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mca_state\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     23\u001B[0m customer_df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mtable(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcustomer\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mc_customer_sk\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mc_current_addr_sk\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     24\u001B[0m store_sales_df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mtable(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstore_sales\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mss_customer_sk\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mss_sold_date_sk\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mss_item_sk\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:473\u001B[0m, in \u001B[0;36mDataFrameReader.table\u001B[0;34m(self, tableName)\u001B[0m\n\u001B[1;32m    439\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtable\u001B[39m(\u001B[38;5;28mself\u001B[39m, tableName: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    440\u001B[0m     \u001B[38;5;124;03m\"\"\"Returns the specified table as a :class:`DataFrame`.\u001B[39;00m\n\u001B[1;32m    441\u001B[0m \n\u001B[1;32m    442\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.4.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    471\u001B[0m \u001B[38;5;124;03m    >>> _ = spark.sql(\"DROP TABLE tblA\")\u001B[39;00m\n\u001B[1;32m    472\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 473\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtable\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtableName\u001B[49m\u001B[43m)\u001B[49m)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `customer_address` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.;\n'UnresolvedRelation [customer_address], [], false\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `customer_address` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.;\n'UnresolvedRelation [customer_address], [], false\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 6. query\n",
    "\n",
    "sfOptions = {\n",
    "  \"sfURL\" : \"https://uh98701.ap-southeast-1.snowflakecomputing.com\",\n",
    "  \"sfAccount\" : \"uh98701.ap-southeast-1\",\n",
    "  \"sfUser\" : \"UMESHCHANDRAKADEM\",\n",
    "  \"sfPassword\" : \"Umesh.K@127\",\n",
    "  \"sfDatabase\" : \"SNOWFLAKE_SAMPLE_DATA\",\n",
    "  \"sfSchema\" : \"TPCDS_SF10TCL\",\n",
    "  \"sfWarehouse\" : \"COMPUTE_WH\"\n",
    "}\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count, col, avg\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Customer Sales Count\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read data from the appropriate sources and create DataFrames\n",
    "customer_address_df = spark.read.table(\"customer_address\").select(\"ca_address_sk\", \"ca_state\")\n",
    "customer_df = spark.read.table(\"customer\").select(\"c_customer_sk\", \"c_current_addr_sk\")\n",
    "store_sales_df = spark.read.table(\"store_sales\").select(\"ss_customer_sk\", \"ss_sold_date_sk\", \"ss_item_sk\")\n",
    "date_dim_df = spark.read.table(\"date_dim\").select(\"d_date_sk\", \"d_month_seq\")\n",
    "item_df = spark.read.table(\"item\").select(\"i_item_sk\", \"i_current_price\", \"i_category\")\n",
    "\n",
    "# Filter date dimension to get the desired month sequence\n",
    "desired_month_seq = date_dim_df.filter((date_dim_df[\"d_year\"] == 2001) & (date_dim_df[\"d_moy\"] == 1)) \\\n",
    "    .select(\"d_month_seq\").distinct()\n",
    "\n",
    "# Calculate the average price per category\n",
    "avg_price_per_category = item_df.join(\n",
    "    item_df.groupBy(\"i_category\").agg(avg(\"i_current_price\").alias(\"avg_price_per_category\")),\n",
    "    \"i_category\"\n",
    ")\n",
    "\n",
    "# Join all tables and apply necessary filters\n",
    "result_df = customer_address_df.join(customer_df, customer_address_df[\"ca_address_sk\"] == customer_df[\"c_current_addr_sk\"]) \\\n",
    "    .join(store_sales_df, customer_df[\"c_customer_sk\"] == store_sales_df[\"ss_customer_sk\"]) \\\n",
    "    .join(date_dim_df, store_sales_df[\"ss_sold_date_sk\"] == date_dim_df[\"d_date_sk\"]) \\\n",
    "    .join(item_df, store_sales_df[\"ss_item_sk\"] == item_df[\"i_item_sk\"]) \\\n",
    "    .join(desired_month_seq, date_dim_df[\"d_month_seq\"] == desired_month_seq[\"d_month_seq\"]) \\\n",
    "    .join(avg_price_per_category, item_df[\"i_category\"] == avg_price_per_category[\"i_category\"]) \\\n",
    "    .where(item_df[\"i_current_price\"] > 1.2 * avg_price_per_category[\"avg_price_per_category\"]) \\\n",
    "    .groupBy(customer_address_df[\"ca_state\"]) \\\n",
    "    .agg(count(\"*\").alias(\"cnt\")) \\\n",
    "    .filter(col(\"cnt\") >= 10) \\\n",
    "    .orderBy(col(\"cnt\"), col(\"ca_state\"))\n",
    "\n",
    "# Show the result\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbe9230b-e1fc-40ca-ad6b-359c03a2be3e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DE_Assignment_New 2024-05-21 22:46:11",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
